= RHTE Multi Cloud Lab Setup Environment

This document explains how to set up the environment for the Multi Cloud Lab.

== Prerequisites

* Register a User ID on https://quay.io. Make sure to use a Red Hat e-mail address when registering.
* A User ID on https://quay.io that is part of the group *coreos*.
** Ask either Kevin Mitts <kmitts@redhat.com> or Kyle Brown <kybrown@redhat.com> for access to the Quay Enterprise image providing your Quay User ID in the e-mail.
+
[TIP]
See https://mojo.redhat.com/docs/DOC-1171579 for detailed instructions
+
* Make sure you can access the Quay Enterprise image
+
[source,bash]
----
docker login quay.io
docker pull quay.io/coreos/quay:v2.9.3
----
+
* Get your Docker Pull Credentials from Quay.io
. Log into https://quay.io
. Click on your user name in the top right corner and select *Account Settings*
. Click on *User Settings* (last icon in the list on the left)
. Under *Docker CLI Password* click on *Generate Encrypted Password* next to *CLI Password*.
. Enter your password when prompted
. In the upcoming dialog select *Docker Configuration* and click the link to *View <your_user>-auth.json*.
. Quay will display a dialog like this:
+
[source,json]
----
{
  "auths": {
    "quay.rhte.example.opentlc.com": {
      "auth": "d2t1bGhhbmsffsf8s2va9Tm1vK0xlQlQ0eWl3dEVQbkVlNjByODZ3cm4vVUhNVEExxx7fsf3fhs0hvU3pyVXRWZDBhWFAza3l4SUQ=",
      "email": ""
    }
  }
}
----
+
. Copy the key next to the *auth* key. Do not copy the enclosing quotes.
+
This key is your authentication key to set up a Quay Enterprise Ansible Agnostic Deployer Configuration.

== Deploy Quay

* Create a *quay-enterprise* configuration.
** Make sure to set *aws_access_key_id*, *aws_secret_access_key*, *qe_quay_registry_auth* and *qe_quay_superuser_password* accordingly.
+
.RC file for the RHTE configuration
[source,bash]
----
GUID=multicloud
REGION=us-east-1
PROFILE=rhte
KEYNAME=ocpkey
ENVTYPE="quay-enterprise"
SOFTWARE_TO_DEPLOY=none
HOSTZONEID='Z2JMY49ICBI99H'
ENVTYPE_ARGS=(
-e repo_version=3.10
-e osrelease=3.10.14
-e own_repo_path=http://admin.na.shared.opentlc.com/repos/ocp/3.10.14
-e "qe_quay_instance_type=m4.2xlarge"
-e "subdomain_base_suffix=.rhte.opentlc.com"
-e "aws_access_key_id=<redacted>"
-e "aws_secret_access_key=<redacted>"
-e "qe_quay_registry_auth=<redacted>"
-e "qe_quay_superuser_username=quayadmin"
-e "qe_quay_superuser_password=<redacted>"
-e "qe_quay_superuser_email=rhpds-admins@redhat.com"
-e '{"cloud_tags":{"owner":"wkulhane@redhat.com"}}'
)
----
+
* Deploy this configuration using AAD.

== Deploy Two OpenShift Clusters

* Create two OpenShift Clusters in the RHTE account on AWS.
** E.g. `master.cloud1.rhte.opentlc.com` and `master.cloud2.rhte.opentlc.com`
** Make sure to provide *aws_access_key_id* and *aws_secret_access_key*.

.RC file for RHTE-CLOUD1
[source,text]
----
GUID=cloud1
REGION=us-east-1
PROFILE=rhte
KEYNAME=ocpkey
ENVTYPE=ocp-workshop
SOFTWARE_TO_DEPLOY=openshift
HOSTZONEID='Z2JMY49ICBI99H'
ENVTYPE_ARGS=(
-e repo_version=3.9
-e osrelease=3.9.40
-e own_repo_path=http://admin.na.shared.opentlc.com/repos/ocp/3.9.40
-e "bastion_instance_type=t2.large"
-e "master_instance_type=m4.4xlarge"
-e "infranode_instance_type=m4.4xlarge"
-e "node_instance_type=m4.4xlarge"
-e "support_instance_type=t2.large"
-e "node_instance_count=2"
-e "install_lets_encrypt_certificates=false"
-e "ovs_plugin=networkpolicy"
-e "container_runtime=docker"
-e "install_openwhisk=false"
-e "install_prometheus=false"
-e "install_glusterfs=false"
-e "glusterfs_device_size=1500"
-e "install_idm=htpasswd"
-e "install_nfs=true"
-e "install_aws_broker=false"
-e "install_openshiftapb=false"
-e "run_ocp_diagnostics=false"
-e "cloudformation_retries=2"
-e "aws_access_key_id=<redacted>"
-e "aws_secret_access_key=<redacted>"
-e "subdomain_base_suffix=.rhte.opentlc.com"
-e "admin_user=wkulhane-redhat.com"
-e '{"cloud_tags":{"owner":"wkulhane@redhat.com"}}'
--skip-tags=remove_self_provisioners,install_zabbix,opentlc_integration
)
----

.RC file for RHTE-CLOUD2
[source,text]
----
GUID=cloud2
REGION=us-east-1
PROFILE=rhte
KEYNAME=ocpkey
ENVTYPE=ocp-workshop
SOFTWARE_TO_DEPLOY=openshift
HOSTZONEID='Z2JMY49ICBI99H'
ENVTYPE_ARGS=(
-e repo_version=3.9
-e osrelease=3.9.40
-e own_repo_path=http://admin.na.shared.opentlc.com/repos/ocp/3.9.40
-e "bastion_instance_type=t2.large"
-e "master_instance_type=m4.4xlarge"
-e "infranode_instance_type=m4.4xlarge"
-e "node_instance_type=m4.4xlarge"
-e "support_instance_type=t2.large"
-e "node_instance_count=2"
-e "install_lets_encrypt_certificates=false"
-e "ovs_plugin=networkpolicy"
-e "container_runtime=docker"
-e "install_openwhisk=false"
-e "install_prometheus=false"
-e "install_glusterfs=false"
-e "glusterfs_device_size=1500"
-e "install_idm=htpasswd"
-e "install_nfs=true"
-e "install_aws_broker=false"
-e "install_openshiftapb=false"
-e "run_ocp_diagnostics=false"
-e "cloudformation_retries=2"
-e "aws_access_key_id=<redacted>"
-e "aws_secret_access_key=<redacted>"
-e "subdomain_base_suffix=.rhte.opentlc.com"
-e "admin_user=wkulhane-redhat.com"
-e '{"cloud_tags":{"owner":"wkulhane@redhat.com"}}'
--skip-tags=remove_self_provisioners,install_zabbix,opentlc_integration
)
----


== Deploy an HA Proxy to load balance the apps domain between the two clusters

* Set up the HA Proxy VM to load balance between the two *.apps domains.
** E.g. `*.apps.rhte-cloud1.example.opentlc.com` and `*.apps.rhte-cloud2.example.opentlc.com`
+
. Create a RHEL 7.5 VM in AWS
* Instance Type: `t2.medium`
* 100GB Root Volume
* Update Security Groups to allow ports 22, 80, 443, 9000
* Use `ocpkey`
. Add an an Elastic IP to the Instance
. Update Route 53 (*rhte.opentlc.com*) configuration in AWS to have two entries that point to the new Elastic IP:
.. *.apps.multicloud.rhte.opentlc.com
.. lb.multicloud.rhte.opentlc.com
. When the RHEL 7.5 VM is running, log into the VM as ec2-user and switch to root (`sudo -i`)
. Update all packages
+
[source,bash]
----
yum -y update
----
+
. Install HAProxy and other tools
+
[source,bash]
----
yum -y install haproxy vim tmux
----
+
. Replace `/etc/haproxy/haproxy.cfg` with this (make sure to replace the two *infranode* servers with the actual Infranodes of the two clusters):
+
[source,text]
----
#---------------------------------------------------------------------
# Global settings
#---------------------------------------------------------------------
global
    maxconn     20000
    log         /dev/log local0
    log         /dev/log local1 notice
    chroot      /var/lib/haproxy
    pidfile     /var/run/haproxy.pid
    user        haproxy
    group       haproxy
    stats socket /var/lib/haproxy/stats
    stats timeout 30s
    daemon

defaults
    log     global
    mode    http
    option  httplog
    option  dontlognull
    timeout connect 5000
    timeout client  50000
    timeout server  50000

listen stats :9000
    mode http
    stats enable
    stats uri /

frontend  http-inbound
    bind  *:80
    mode http
    option  http-server-close
    default_backend http-outbound

backend http-outbound
    mode    http
    balance roundrobin
    option  forwardfor
    option  http-server-close
    http-request set-header X-Forwarded-Port %[dst_port]
    http-request add-header X-Forwarded-Proto https if { ssl_fc }
    server cluster1 infranode.cloud1.rhte.opentlc.com:80 check
    server cluster2 infranode.cloud2.rhte.opentlc.com:80 check
----
+
. Enable and start HA Proxy
+
[source,bash]
----
systemctl enable haproxy
systemctl start haproxy
----
+
. This is it. Students can now expose their applications as `xyz.apps.rhte.example.opentlc.com` and be routed to their applications.
