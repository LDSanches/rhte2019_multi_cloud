:toc2:
:linkattrs:

= Red Hat Tech Exchange 2019

== A0007 Multi-Cloud OpenShift Lab

Welcome to the 2019 Multi-Cloud OpenShift^(R)^ lab.

You may remember that at Red Hat^(R)^ Tech Exchange (RHTE) 2018 we ran a similar lab. Federation did not exist yet, and the only pipeline technology available was Jenkins. So we did everything manually from a Jenkins pipeline.

Now, just one year later, we can use Kubernetes and OpenShift technology to perform many of these tasks instead of having to do everything in a Jenkins pipeline.

=== Goals

In this lab, you will:

* Use *KubeFed* to set up a federated cluster consisting of two OpenShift clusters
* Deploy an application to two clusters using federated API objects
* Register for an account on a private *Quay* container registry
* Use *OpenShift Pipelines*--which are based on *Tekton Pipelines*--to build an updated container image for the application and redeploy it to both clusters using KubeFed and the Quay image registry

[WARNING]
Both KubeFed and OpenShift are Developer Preview at the moment. They can and will change before they are released. In addition, both products currently require a *cluster administrator* account. Hopefully, this will change before the final release.

=== Infrastructure

You have one primary cluster where you will do all of the work. On that cluster, you have cluster-admin privileges. That cluster already has KubeFed deployed. Only the source cluster needs KubeFed deployed--all other clusters are managed from the primary cluster. This makes adding clusters to a federated cluster very easy.

There is a second, shared, cluster for all students to deploy your applications to.

=== Prerequisites

. Get a GUID from the GUIDGrabber. This is the GUID of your personal cluster.
* Cluster 1 has a bastion VM that you use to execute all tasks.
* GUIDGrabber will show the command to connect to the bastion VM.
* Your cluster runs in a Sandbox. Make a note of the Sandbox Number.
** For example, if GUIDGrabber shows the following command:
+
[source,sh]
----
ssh lab-user@bastion.b1fa.sandbox23.opentlc.com
----
+
this means that your GUID is `b1fa` and your Sandbox Number is `23`.
. There is a second, shared, cluster. You do _not_ receive a GUID for the second cluster.

=== Log into Bastion VM for Cluster 1

. Use any SSH client of your choice to log into the Bastion VM of your personal cluster:
* User ID: `lab-user`
* Password: `rhte2019`
+
[source,sh]
----
ssh lab-user@bastion.<GUID>.sandbox<SANDBOX_NUMBER>.opentlc.com
----

. Double-check that your GUID variable is set correctly:
+
[source,sh]
----
echo ${GUID}
----

. Set up two variables for the two clusters and save them in `.bashrc`:
+
[source,sh]
----
export CLUSTER1=$(oc whoami --show-server | cut -f 2 -d ':' | cut -f 3 -d '/'  | sed 's/api.//')
export CLUSTER2=cluster-common.common.events.opentlc.com
echo "export CLUSTER1=$(oc whoami --show-server | cut -f 2 -d ':' | cut -f 3 -d '/'  | sed 's/api.//')" >>$HOME/.bashrc
echo "export CLUSTER2=cluster-common.common.events.opentlc.com" >>$HOME/.bashrc
----

. Log into OpenShift as `admin` with password `rhte2019`. This is a user with full cluster administration privileges (but not system:admin):
+
[source,sh]
----
oc login -u admin -p rhte2019 --insecure-skip-tls-verify https://api.${CLUSTER1}:6443
----
+
.Sample Output
[source,texinfo]
----
Login successful.

You have access to the following projects and can switch between them with `oc project <projectname>`:

  * default
    kube-federation-system
[...]

Using project "default".
----

[NOTE]
====
To save resources, your personal cluster consists of only one master and one worker node. This is obviously not a supported configuration for a production cluster, but it is sufficient for the purposes of this lab.
====

== Set Up Federated Cluster

KubeFed requires clusters to be registered. The way this works is that an initial federated cluster is created. Then a second cluster is joined to this initial cluster.

After this step is completed, you can create a federated project to deploy your federated application into. KubeFed takes care of distributing the Kubernetes objects that make up this application to every cluster in the federated cluster.

. Validate the Kubernetes context you are using:
+
[source,sh]
----
oc config get-contexts
----
+
.Sample Output
[source,texinfo,options=nowrap]
----
CURRENT   NAME                                                             CLUSTER                                            AUTHINFO                                                 NAMESPACE
          admin                                                            cluster-b1fa                                       admin
*         default/api-cluster-b1fa-b1fa-sandbox23-opentlc-com:6443/admin   api-cluster-b1fa-b1fa-sandbox23-opentlc-com:6443   admin/api-cluster-b1fa-b1fa-sandbox23-opentlc-com:6443   default
----
+
You use the `default/api-cluster-b1fa-b1fa-sandbox23-opentlc-com:6443/admin` context for your initial cluster.
. The context name is a bit difficult to remember. To make it easier, rename the context to `cluster1-${GUID}`:
+
[source,sh]
----
oc config rename-context $(oc config current-context) cluster1-${GUID}
----
+
.Sample Output
[source,texinfo]
----
Context "default/api-cluster-b1fa-b1fa-sandbox23-opentlc-com:6443/admin" renamed to "cluster1-b1fa".
----

. To join the clusters, you need to also create a context for the second cluster. By logging into the second cluster, the `$HOME/.kube/config` file in your first cluster VM is updated with the context for the second cluster. That context contains the information about how to access the second cluster.
+
Log into the second cluster as user `admin`:
+
[WARNING]
====
Do not run any commands not listed in this lab on cluster 2. This is a shared cluster, and you might break things for your fellow students.
====
+
[source,sh]
----
oc login -u admin -p rhte2019 --insecure-skip-tls-verify https://api.${CLUSTER2}:6443
----

. Now that you are logged into the second cluster, your _local_ Kube config file in the cluster 1 bastion VM has been updated with the context for cluster 2.
. Once again, rename the current context to `cluster2-${GUID}`:
+
[source,sh]
----
oc config rename-context $(oc config current-context) cluster2-${GUID}
----
+
Verify that the context for cluster 2 is now available:
+
[source,sh]
----
oc config get-contexts
----
+
.Sample Output
[source,texinfo,options=nowrap]
----
CURRENT   NAME            CLUSTER                                             AUTHINFO                                                  NAMESPACE
          admin           cluster-b1fa                                        admin
          cluster1-b1fa   api-cluster-b1fa-b1fa-sandbox23-opentlc-com:6443    admin/api-cluster-b1fa-b1fa-sandbox23-opentlc-com:6443    default
*         cluster2-b1fa   api-cluster-common-common-events-opentlc-com:6443   admin/api-cluster-common-common-events-opentlc-com:6443   default
----

. Switch your active context back to cluster 1. This is the same as logging back into the first cluster:
+
[source,sh]
----
oc config use-context cluster1-${GUID}
----

. You now have easy access to the context for both cluster 1 and cluster 2.
+
Create the initial federated cluster:
+
[source,sh]
----
kubefedctl join cluster1-${GUID} --host-cluster-context cluster1-${GUID} --cluster-context cluster1-${GUID} --v=2
----
+
.Sample Output
[source,texinfo]
----
I0814 08:12:13.384334   23391 join.go:159] Args and flags: name cluster1-b1fa, host: cluster1-b1fa, host-system-namespace: kube-federation-system, kubeconfig: , cluster-context: cluster1-b1fa, secret-name: , dry-run: false
I0814 08:12:13.564995   23391 join.go:219] Performing preflight checks.
I0814 08:12:13.566980   23391 join.go:225] Creating kube-federation-system namespace in joining cluster
I0814 08:12:13.569479   23391 join.go:352] Already existing kube-federation-system namespace
I0814 08:12:13.569495   23391 join.go:233] Created kube-federation-system namespace in joining cluster
I0814 08:12:13.569509   23391 join.go:236] Creating cluster credentials secret
I0814 08:12:13.569595   23391 join.go:372] Creating service account in joining cluster: cluster1-b1fa
I0814 08:12:13.576169   23391 join.go:382] Created service account: cluster1-b1fa-cluster1-b1fa in joining cluster: cluster1-b1fa
I0814 08:12:13.576185   23391 join.go:410] Creating cluster role and binding for service account: cluster1-b1fa-cluster1-b1fa in joining cluster: cluster1-b1fa
I0814 08:12:13.589596   23391 join.go:419] Created cluster role and binding for service account: cluster1-b1fa-cluster1-b1fa in joining cluster: cluster1-b1fa
I0814 08:12:13.589616   23391 join.go:423] Creating secret in host cluster: cluster1-b1fa
I0814 08:12:14.600195   23391 join.go:812] Using secret named: cluster1-b1fa-cluster1-b1fa-token-r7vc2
I0814 08:12:14.602977   23391 join.go:855] Created secret in host cluster named: cluster1-b1fa-4jjz8
I0814 08:12:14.602993   23391 join.go:432] Created secret in host cluster: cluster1-b1fa
I0814 08:12:14.603004   23391 join.go:246] Cluster credentials secret created
I0814 08:12:14.603029   23391 join.go:248] Creating federated cluster resource
I0814 08:12:14.609625   23391 join.go:257] Created federated cluster resource
----

. Validate that the cluster is now registered as a federated cluster:
+
[source,sh]
----
oc get kubefedclusters -n kube-federation-system
----
+
.Sample Output
[source,texinfo]
----
NAME            READY   AGE
cluster1-b1fa   True    35s
----
+
If the value in column `READY` is not yet `True`, repeat the command until it is.

. Describe the federated cluster:
+
[source,sh]
----
oc describe kubefedcluster cluster1-${GUID}  -n kube-federation-system
----
+
.Sample Output
[source,texinfo]
----
Name:         cluster1-b1fa
Namespace:    kube-federation-system
Labels:       <none>
Annotations:  <none>
API Version:  core.kubefed.k8s.io/v1beta1
Kind:         KubeFedCluster
Metadata:
  Creation Timestamp:  2019-08-14T08:12:14Z
  Generation:          1
  Resource Version:    21889
  Self Link:           /apis/core.kubefed.k8s.io/v1beta1/namespaces/kube-federation-system/kubefedclusters/cluster1-b1fa
  UID:                 3971eefb-be6b-11e9-a879-06e77dfe2d88
Spec:
  API Endpoint:  https://api.cluster-b1fa.b1fa.sandbox23.opentlc.com:6443

[...]

Status:
  Conditions:
    Last Probe Time:       2019-08-14T08:12:58Z
    Last Transition Time:  2019-08-14T08:12:58Z
    Message:               /healthz responded with ok
    Reason:                ClusterReady
    Status:                True
    Type:                  Ready
  Region:                  ap-southeast-1
  Zones:
    ap-southeast-1a
Events:  <none>
----
+
// Unjoin if necessary
// kubefedctl unjoin cluster2 --host-cluster-context cluster1 --cluster-context cluster2 --v=2

. Join the second cluster to the first cluster to create your federated environment:
+
[source,sh]
----
kubefedctl join cluster2-${GUID} --host-cluster-context cluster1-${GUID} --cluster-context cluster2-${GUID} --v=2
----
+
.Sample Output
[source,texinfo]
----
I0814 08:13:33.489975   23438 join.go:159] Args and flags: name cluster2-b1fa, host: cluster1-b1fa, host-system-namespace: kube-federation-system, kubeconfig: , cluster-context: cluster2-b1fa, secret-name: , dry-run: false
I0814 08:13:33.925875   23438 join.go:219] Performing preflight checks.
I0814 08:13:35.094411   23438 join.go:225] Creating kube-federation-system namespace in joining cluster
I0814 08:13:35.555457   23438 join.go:233] Created kube-federation-system namespace in joining cluster
I0814 08:13:35.555482   23438 join.go:236] Creating cluster credentials secret
I0814 08:13:35.555498   23438 join.go:372] Creating service account in joining cluster: cluster2-b1fa
I0814 08:13:35.785497   23438 join.go:382] Created service account: cluster2-b1fa-cluster1-b1fa in joining cluster: cluster2-b1fa
I0814 08:13:35.785519   23438 join.go:410] Creating cluster role and binding for service account: cluster2-b1fa-cluster1-b1fa in joining cluster: cluster2-b1fa
I0814 08:13:36.707235   23438 join.go:419] Created cluster role and binding for service account: cluster2-b1fa-cluster1-b1fa in joining cluster: cluster2-b1fa
I0814 08:13:36.707257   23438 join.go:423] Creating secret in host cluster: cluster1-b1fa
I0814 08:13:37.394448   23438 join.go:812] Using secret named: cluster2-b1fa-cluster1-b1fa-token-xzndg
I0814 08:13:37.400751   23438 join.go:855] Created secret in host cluster named: cluster2-b1fa-tpls2
I0814 08:13:37.400769   23438 join.go:432] Created secret in host cluster: cluster1-b1fa
I0814 08:13:37.400781   23438 join.go:246] Cluster credentials secret created
I0814 08:13:37.400790   23438 join.go:248] Creating federated cluster resource
I0814 08:13:37.412103   23438 join.go:257] Created federated cluster resource
----

. Once again, verify that the cluster is ready, and describe the properties of the cluster:
+
[source,sh]
----
oc get kubefedclusters -n kube-federation-system
----
+
.Sample Output
[source,texinfo]
----
NAME            READY   AGE
cluster1-b1fa   True    102s
cluster2-b1fa   True    19s
----
+
[source,sh]
----
oc describe kubefedcluster cluster2-${GUID} -n kube-federation-system
----

. Your clusters are ready to receive and distributed federated resources. The setup for this lab already registered four types with the Kube Federation system:
+
[options=header]
|====
|Original Resource|Federated Resource
|Namespace|FederatedNamespace
|Deployment|FederatedDeployment
|Service|FederatedService
|Ingress|FederatedIngress
|====
+
After being registered, the cluster now understands the federated type. If you create a federated resource, it is automatically distributed over all of the clusters.
+
[TIP]
You can enable additional API types using the `kubefedctl enable <type>` command--for example, `kubefedctl enable PersistentVolumeClaim`.

== Set Up Federated Project and Federated Application

. Start by creating a federated project.
. Create a directory for the YAML manifests:
+
[source,sh]
----
mkdir $HOME/rhte-app
cd $HOME/rhte-app
----

. Create a project on your first cluster:
+
[source,sh]
----
oc new-project rhte-app-${GUID} --display-name="RHTE 2019 Multi-Cloud Lab for GUID ${GUID}"
----
+
.Sample Output
[source,texinfo,options=nowrap]
----
Now using project "rhte-app-b1fa" on server "https://api.cluster-b1fa.b1fa.sandbox23.opentlc.com:6443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app django-psql-example

to build a new example application in Python. Or use kubectl to deploy a simple Kubernetes application:

    kubectl create deployment hello-node --image=gcr.io/hello-minikube-zero-install/hello-node
----

. When the project exists, use `kubefedctl` to federate the project:
+
[source,sh]
----
kubefedctl federate namespace rhte-app-${GUID}
----
+
.Sample Output
[source,texinfo,options=nowrap]
----
W0918 03:00:59.953183   17708 federate.go:410] Annotations defined for Namespace "rhte-app-52b2" will not appear in the template of the federated resource: map[openshift.io/description: openshift.io/display-name:RHTE 2019 Multi-Cloud Lab for GUID 52b2 openshift.io/requester:admin openshift.io/sa.scc.mcs:s0:c23,c12 openshift.io/sa.scc.supplemental-groups:1000530000/10000 openshift.io/sa.scc.uid-range:1000530000/10000]
I0918 03:00:59.953287   17708 federate.go:474] Resource to federate is a namespace. Given namespace will itself be the container for the federated namespace
I0918 03:00:59.956719   17708 federate.go:503] Successfully created FederatedNamespace "rhte-app-52b2/rhte-app-52b2" from Namespace
----

// [NOTE]
// ====
// You could have also created the `FederatedNamespace` from a YAML definition. In the next few steps, you use the YAML approach. Using `kubefedctl federate` is a convenient way to federate resources that already exist.

// . Create the Federated Namespace YAML manifest:
// +
// [source,sh]
// ----
// cat << EOF >$HOME/rhte-app/namespace.yaml
// apiVersion: types.kubefed.io/v1beta1
// kind: FederatedNamespace
// metadata:
//   name: rhte-app-${GUID}
//   namespace: rhte-app-${GUID}
// spec:
//   placement:
//     clusterSelector:
//       matchLabels: {}
//   template:
//     spec: {}
// EOF
// ----

// . Create the Namespace.
// +
// [source,sh]
// ----
// oc create namespace rhte-app-${GUID}
// ----
// +
// .Sample Output
// [source,texinfo]
// ----
// namespace/rhte-app-wk created
// ----

// . Create the Federated Namespace.
// +
// [source,sh]
// ----
// oc apply -f $HOME/rhte-app/namespace.yaml -n rhte-app-${GUID}
// ----
// +
// .Sample Output
// [source,texinfo]
// ----
// federatednamespace.types.kubefed.io/rhte-app-xxxx created
// ----
// ====

. Create the Federated Deployment for the application:
+
[source,sh]
----
cat << EOF >$HOME/rhte-app/deployment.yaml
apiVersion: types.kubefed.io/v1beta1
kind: FederatedDeployment
metadata:
  name: rhte-app
spec:
  template:
    metadata:
      name: rhte-app
      labels:
        name: rhte-app
    spec:
      selector:
        matchLabels:
          name: rhte-app
      replicas: 1
      template:
        metadata:
          labels:
            name: rhte-app
        spec:
          containers:
          - name: rhte-app
            image: quay.io/wkulhanek/rhte-placeholder:latest
            ports:
            - containerPort: 3000
            env:
            - name: CLUSTER_NAME
              value: "To be overwritten"
            - name: IMAGE_TAG
              value: "To be overwritten"
            - name: PREFIX
              value: "To be overwritten"
  placement:
    clusters:
    - name: cluster1-${GUID}
    - name: cluster2-${GUID}
  overrides:
  - clusterName: cluster1-${GUID}
    clusterOverrides:
    - path: /spec/template/spec/containers/0/env/0/value
      value: "Cluster 1"
    - path: /spec/template/spec/containers/0/env/2/value
      value: $GUID
  - clusterName: cluster2-${GUID}
    clusterOverrides:
    - path: /spec/template/spec/containers/0/env/0/value
      value: "Cluster 2"
    - path: /spec/template/spec/containers/0/env/2/value
      value: "common"
EOF
----

. Note the following:
* Under `spec.template.spec.template`, you will find the original Deployment definition. It contains metadata, the spec with the container definition, and a few environment variables.
** The image that gets deployed is `quay.io/wkulhanek/rhte-placeholder:latest`. It does not have the capability to read environment variables. You will update to a proper container image when writing the pipeline.
* `placement` specifies that this deployment is to be placed on both clusters, `cluster1` and `cluster2`.
* The application that you are using understands a few environment variables and shows the value of the environment variables as a web page. To specify the correct environment variable for each cluster, the `overrides` section specifies specific values for each cluster.
+
For example, on cluster 1 the `CLUSTER_NAME` environment variable will be set to `Cluster 1`, while on cluster 2 it will be set to `Cluster 2`.

. Now create the Federated Deployment:
+
[source,sh]
----
oc apply -f $HOME/rhte-app/deployment.yaml -n rhte-app-${GUID}
----
+
.Sample Output
[source,texinfo]
----
federateddeployment.types.kubefed.io/rhte-app created
----

. Validate that both the Federated Deployment and the Deployment now exist:
+
[source,sh]
----
oc get federateddeployments,deployments -n rhte-app-${GUID}
----
+
.Sample Output
[source,texinfo]
----
NAME                                            AGE
federateddeployment.types.kubefed.io/rhte-app   46s

NAME                             READY   UP-TO-DATE   AVAILABLE   AGE
deployment.extensions/rhte-app   1/1     1            0           44s
----

. An application needs the networking resources to be accessible. Create the definition for the federated service:
+
[source,sh]
----
cat << EOF >$HOME/rhte-app/service.yaml
apiVersion: types.kubefed.io/v1beta1
kind: FederatedService
metadata:
  name: rhte-app
spec:
  template:
    spec:
      selector:
        name: rhte-app
      ports:
        - name: http
          port: 3000
  placement:
    clusters:
    - name: cluster1-${GUID}
    - name: cluster2-${GUID}
EOF
----

. Again, note that `spec.template.spec` contains the information you usually would see in a `service` object.
. Create the federated service:
+
[source,sh]
----
oc apply -f $HOME/rhte-app/service.yaml -n rhte-app-${GUID}
----
+
.Sample Output
[source,texinfo]
----
federatedservice.types.kubefed.io/rhte-app created
----

. Finally, you need to create a Route to make the application accessible from the Internet. This lab uses standard Kubernetes objects--therefore, you create an `Ingress` resource, which OpenShift automatically converts into a `Route`.
+
Create the YAML definition of the `FederatedIngress` resource:
+
[source,sh]
----
cat << EOF >$HOME/rhte-app/ingress.yaml
apiVersion: types.kubefed.io/v1beta1
kind: FederatedIngress
metadata:
  name: rhte-app
spec:
  template:
    spec:
      rules:
      - host: rhte-app
        http:
          paths:
          - path: /
            backend:
              serviceName: rhte-app
              servicePort: 3000
  placement:
    clusters:
    - name: cluster1-${GUID}
    - name: cluster2-${GUID}
  overrides:
  - clusterName: cluster1-${GUID}
    clusterOverrides:
    - path: /spec/rules/0/host
      value: rhte-app-${GUID}.apps.${CLUSTER1}
  - clusterName: cluster2-${GUID}
    clusterOverrides:
    - path: /spec/rules/0/host
      value: rhte-app-${GUID}.apps.${CLUSTER2}
EOF
----

. Again, note the following:
* `spec.template.spec` contains the usual fields you would expect to see in a Kubernetes Ingress resource.
* `placement` once again specifies that both clusters are to receive this ingress object--and therefore, the route.
* `overrides` specifies the hostname for the ingress object. This is necessary because the default subdomain is different on both clusters. Therefore, you need to explicitly set the hostname.

. Create the `FederatedIngress` resource:
+
[source,sh]
----
oc apply -f $HOME/rhte-app/ingress.yaml -n rhte-app-${GUID}
----
+
.Sample Output
[source,texinfo]
----
federatedingress.types.kubefed.io/rhte-app created
----

. Validate that in fact both an `ingress` and `route` resource were created:
+
[source,sh]
----
oc get ingresses,routes -n rhte-app-${GUID}
----
+
.Sample Output
[source,texinfo,options=nowrap]
----
NAME                          HOSTS                                                        ADDRESS   PORTS   AGE
ingress.extensions/rhte-app   rhte-app-b1fa.apps.cluster-b1fa.b1fa.sandbox23.opentlc.com             80      11s

NAME                                      HOST/PORT                                                    PATH   SERVICES   PORT   TERMINATION   WILDCARD
route.route.openshift.io/rhte-app-jqzv4   rhte-app-b1fa.apps.cluster-b1fa.b1fa.sandbox23.opentlc.com   /      rhte-app   3000                 None
----

. In a browser window, navigate to the route displayed--in the example above, `rhte-app-b1fa.apps.cluster-b1fa.b1fa.sandbox23.opentlc.com`&#8212;and validate that the application works and does not tell you which cluster it is running on. You should see `Placeholder for` for all three lines of text.

. As a final step, verify that the application is running in the second cluster as well.
+
Log back into the second cluster:
+
[source,sh]
----
oc config use-context cluster2-${GUID}
----
. Display all resources in the `rhte-app-${GUID}` project. Note that you never created the project in cluster 2--but by federating the namespace, the project was created in cluster 2, as well:
+
[source,sh]
----
oc get all,ingresses -n rhte-app-${GUID}
----
+
.Sample Output
[source,texinfo,options=nowrap]
----
NAME                            READY   STATUS    RESTARTS   AGE
pod/rhte-app-5895bfcf6c-z8pxh   1/1     Running   0          4m22s

NAME               TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
service/rhte-app   ClusterIP   172.30.193.210   <none>        3000/TCP   2m9s

NAME                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/rhte-app   1/1     1            1           4m22s

NAME                                  DESIRED   CURRENT   READY   AGE
replicaset.apps/rhte-app-5895bfcf6c   1         1         1       4m23s

NAME                                      HOST/PORT                                                     PATH   SERVICES   PORT   TERMINATION   WILDCARD
route.route.openshift.io/rhte-app-zhlmf   rhte-app-b1fa.apps.cluster-common.common.events.opentlc.com   /      rhte-app   3000                 None

NAME                          HOSTS                                                         ADDRESS   PORTS   AGE
ingress.extensions/rhte-app   rhte-app-b1fa.apps.cluster-common.common.events.opentlc.com             80      93s
----
. Note that all resources are available in cluster 2 as well, and that the route and ingress point to the domain in cluster 2.

. Validate that the deployment has been updated with environment variables for cluster 2, as well (remember the `overrides` section in the original federated deployment):
+
[source,sh]
----
oc set env deployment rhte-app -n rhte-app-${GUID} --list
----
+
.Sample Output
[source,texinfo,options=nowrap]
----
# deployments/rhte-app, container rhte-app
CLUSTER_NAME=Cluster 2
IMAGE_TAG=To be overwritten
PREFIX=common
----

. Log back into cluster 1:
+
[source,sh]
----
oc config use-context cluster1-${GUID}
----

Your federated project is now set up and ready to be used in the pipeline.


== Create Tekton Pipeline

Now that the application is ready, you can set up a pipeline to do the following:

* Build a container image from a GitHub repository
* Tag the container image with a Tag
* Copy the container image into an external registry to make it accessible from both clusters
* Update the Federated Deployment to update the deployments on both clusters with the new container image

OpenShift Pipelines is a fully Kubernetes-native pipeline implementation. It is under heavy development, and it does not yet have a Graphical User Interface for building, running, and managing pipelines. On OpenShift 4, the pipelines are managed using the *OpenShift Pipeline Operator*. This operator has already been deployed into your primary cluster.

[TIP]
You can find a tutorial for OpenShift Pipelines at link:https://github.com/openshift/pipelines-tutorial[https://github.com/openshift/pipelines-tutorial^].

Pipelines consist of *Tasks* and *Pipelines*. Both tasks and pipelines are designed to be reusable. To run a task you create a *TaskRun*, and to run a pipeline you create a *PipelineRun*. Both TaskRuns and PipelineRuns can pass parameters into the tasks and pipelines to influence the build steps.

Common *PipelineResources* consist of Git repositories or container image locations.

=== Register Quay Account

In this lab, you use the Quay registry to hold the container images for your application.

If you do not have a Quay account, you need to register for one. If you already have a Quay account, log into Quay, skip this section, and go to the next section to create a Quay repository.

. In a web browser, navigate to link:https://quay-common.apps.cluster-common.common.events.opentlc.com[https://quay-common.apps.cluster-common.common.events.opentlc.com^].
* This is a private Quay container image registry set up just for this lab. It is being managed by the link:https://github.com/redhat-cop/quay-operator[Quay Operator].
. On the Quay home page, click *Create Account* below the login entry fields.
. Pick a username, specify your email address, and pick a password. Then click *Create Account*.
** Your email address will not be used for anything.
Your account will be created, and you will be logged into Quay.

=== Create Quay Repository

You now create a public repository in Quay that you use to push your container images to.

. Click the *Create New Repository* link toward the top right of the Quay page.
. Use *rhte-app* as the name of the repository.
. Make sure you select *Public* for the type of repository.
. Click *Create Public Repository*.

=== Create Quay Robot Account

While your image repository is public, you need credentials to access Quay from the pipeline to push images. It is generally a bad idea to use your own user ID and password. Fortunately, Quay has a mechanism to create a *Robot Account*, which can easily be updated or revoked if necessary.

. In the Quay web interface, click your account name in the top right corner, then select *Account Settings*.
. On the left, click the second icon from the top--the one that looks like a robot. Then, on the right, click *Create Robot Account*.
. In the entry field, use *rhte* as the name for the new robot account. Optionally, add a description. Then click *Create robot account*.
. When prompted for permissions, select the *rhte-app* repository by checking the box to the left of it. Change the *Permission* dropdown to *Write*. Then click *Add Permission*.
. For your newly created robot account, note your account name and the name of the robot account--e.g., *wkulhanek+rhte*.
. Click the settings icon--the one that looks like a gear--to the far right of your robot account, and select *View Credentials*.
. Make sure to save both your robot account *Username* (e.g., *wkulhanek+rhte*) and the *Robot Token*. (You may want to write them into a text file.) You need these credentials later in the lab.
. After you have copied the username and token, you may close the Quay window.

=== Set Up Pipeline Project

. From the bastion VM, create a project to hold the pipeline:
+
[source,sh]
----
oc new-project rhte-pipeline --display-name="RHTE 2019 OpenShift Pipeline"
----

. Create a directory to hold all of the YAML files representing the various resources that make up the pipeline:
+
[source,sh]
----
mkdir $HOME/pipeline
cd $HOME/pipeline
----

. Create a *Secret* YAML manifest to store the Quay Robot Account credentials. Make sure to use _your_ robot account and token:
+
[source,sh]
----
export QUAY_ACCOUNT=< Quay Robot Account >
export QUAY_TOKEN=< Quay Robot Token >

cat << EOF >$HOME/pipeline/quay-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: quay-credentials
  annotations:
    tekton.dev/docker-0: https://quay-common.apps.cluster-common.common.events.opentlc.com
type: kubernetes.io/basic-auth
stringData:
  # Create Robot Account with Write Permissions at https://quay.io
  username: $QUAY_ACCOUNT
  password: $QUAY_TOKEN
EOF
----

. Create the secret in the pipeline project:
+
[source,sh]
----
oc apply -f $HOME/pipeline/quay-secret.yaml -n rhte-pipeline
----
+
.Sample Output
[source,texinfo]
----
secret/quay-credentials created
----

. Pipelines need a service account with permissions to run privileged pods--especially build pods. But because the pipeline also needs to update the KubeFed objects, you need to grant cluster-admin permissions to the pipeline service account. Note that this would not be recommended in a production system, and hopefully a future release of KubeFed will no longer require cluster-admin permission.
+
The service account also needs to be linked to the Quay credentials secret that you just created.
+
Create the service account definition:
+
[source,sh]
----
cat << EOF >$HOME/pipeline/pipeline-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: pipeline
secrets:
  - name: quay-credentials
EOF
----
. Create the service account:
+
[source,sh]
----
oc apply -f pipeline-serviceaccount.yaml -n rhte-pipeline
----
+
.Sample Output
[source,texinfo]
----
serviceaccount/pipeline created
----
. Now grant the proper permissions to the service account. The service account needs two roles:
* `privileged`: This is necessary to run builds inside a container.
* `cluster-admin`: This is necessary for the pipeline to be able to update KubeFed resources. In the future it will be possible to use much more restrictive permissions.
+
[source,sh]
----
oc adm policy add-scc-to-user privileged -z pipeline -n rhte-pipeline
oc adm policy add-cluster-role-to-user cluster-admin system:serviceaccount:rhte-pipeline:pipeline
----
+
.Sample Output
[source,texinfo]
----
securitycontextconstraints.security.openshift.io/privileged added to: ["system:serviceaccount:rhte-pipeline:pipeline"]
clusterrole.rbac.authorization.k8s.io/cluster-admin added: "system:serviceaccount:rhte-pipeline:pipeline"
----

=== Set Up Tasks

The first step in setting up a pipeline is to create all of the task definitions that the pipeline will use.

The pipeline for this lab uses the following tasks:

* S2I NodeJS Build
* OpenShift CLI: For tagging
* Skopeo: To move the container image to Quay
* OpenShift Patch: To update the Federated Deployment with the new image location

Both the link:https://github.com/tektoncd/catalog[Tekton GitHub repository^] and the link:https://github.com/openshift/pipelines-catalog[OpenShift Pipelines GitHub repository^] have a catalog of available tasks.

. Create the S2I NodeJS task:
+
[source,sh]
----
oc apply -f https://raw.githubusercontent.com/openshift/pipelines-catalog/master/s2i-nodejs/s2i-nodejs-task.yaml -n rhte-pipeline
----
+
.Sample Output
[source,texinfo]
----
task.tekton.dev/s2i-nodejs created
----

. Create the OpenShift CLI task manifest YAML file:
+
[source,sh]
----
cat << EOF >$HOME/pipeline/task-openshift.yaml
apiVersion: tekton.dev/v1alpha1
kind: Task
metadata:
  name: openshift-client
spec:
  inputs:
    params:
      - name: ARGS
        description: The OpenShift CLI arguments to run
        default: help
  steps:
    - name: oc
      image: quay.io/gpte-devops-automation/tekton-openshift-cli:0.5.2
      command: ["/usr/local/bin/oc"]
      args:
        - "\${inputs.params.ARGS}"
EOF
----

. Then create the task:
+
[source,sh]
----
oc apply -f $HOME/pipeline/task-openshift.yaml -n rhte-pipeline
----
+
.Sample Output
[source,texinfo]
----
task.tekton.dev/openshift-client created
----

// . Create the OpenShift CLI task:
// +
// [source,sh]
// ----
// oc apply -f https://raw.githubusercontent.com/tektoncd/catalog/master/openshift-client/openshift-client-task.yaml -n rhte-pipeline
// ----
// +
// .Sample Output
// [source,texinfo]
// ----
// task.tekton.dev/openshift-client created
// ----

. You need a task to copy the image from the integrated OpenShift registry to an external registry, which in this case is Quay. There is a container image for this task already available. All you need to create is the task definition.
+
Create the task manifest YAML file:
+
[source,sh]
----
cat << EOF >$HOME/pipeline/task-skopeo.yaml
apiVersion: tekton.dev/v1alpha1
kind: Task
metadata:
  name: skopeo
spec:
  inputs:
    params:
    - name: ARGS
      description: The skopeo CLI arguments to run
      default: --help
  steps:
  - name: skopeo
    image: quay.io/gpte-devops-automation/tekton-skopeo:0.1
    command: ["/usr/local/bin/skopeo"]
    args:
      - "\${inputs.params.ARGS}"
EOF
----
. Then create the task:
+
[source,sh]
----
oc apply -f $HOME/pipeline/task-skopeo.yaml -n rhte-pipeline
----
+
.Sample Output
[source,texinfo]
----
task.tekton.dev/skopeo created
----

. Finally, create a task to patch a resource in OpenShift. The generic OpenShift CLI task does not work for this use case:
+
[source,sh]
----
cat << EOF >$HOME/pipeline/task-patch.yaml
apiVersion: tekton.dev/v1alpha1
kind: Task
metadata:
  name: patch
spec:
  inputs:
    params:
    - name: RESOURCE
      description: The resource (e.g. deployment, federateddeployment, ...) to updated
    - name: RESOURCE_NAME
      description: The name of the resource to be patched
    - name: NAMESPACE
      description: The Namespace that has the Federated Deployment
    - name: PATCH
      description: The patch string to use
    - name: TYPE
      description: The type of patch
      default: strategic
  steps:
  - name: patch
    image: quay.io/gpte-devops-automation/tekton-openshift-cli:0.5.2
    command: ['/usr/local/bin/oc-origin', 'patch', '\${inputs.params.RESOURCE}', '\${inputs.params.RESOURCE_NAME}', '-n', '\${inputs.params.NAMESPACE}', '--type', '\${inputs.params.TYPE}', '--patch', '\${inputs.params.PATCH}']
EOF
----
. Create the task:
+
[source,sh]
----
oc apply -f $HOME/pipeline/task-patch.yaml -n rhte-pipeline
----
+
.Sample Output
[source,texinfo]
----
task.tekton.dev/patch created
----

. Verify that all four tasks are now registered:
+
[source,sh]
----
oc get tasks
----
+
.Sample Output
[source,texinfo]
----
NAME               AGE
openshift-client   11m
patch              2m59s
s2i-nodejs         12m
skopeo             8m14s
----

=== Set Up Pipeline Resources

Since Pipelines are supposed to be generic, you need a way to provide the parameters to the pipeline and the tasks that make up the pipeline. This is implemented using *PipelineResource* resources.

In this lab, you use two resources--the Git repository with the source code and the name and tag of the container image to be built.

. Create the `PipelineResource` definition for the Git Repository:
+
[source,sh]
----
cat << EOF >$HOME/pipeline/rhte-git.yaml
apiVersion: tekton.dev/v1alpha1
kind: PipelineResource
metadata:
  name: rhte-git
spec:
  type: git
  params:
  - name: url
    value: https://github.com/wkulhanek/rhte-app.git
EOF
----
. Create the Git PipelineResource:
+
[source,sh]
----
oc apply -f $HOME/pipeline/rhte-git.yaml -n rhte-pipeline
----
+
.Sample Output
[source,texinfo]
----
pipelineresource.tekton.dev/rhte-git created
----

. Create the `PipelineResource` definition for the container image:
+
[source,sh]
----
cat << EOF >$HOME/pipeline/rhte-image.yaml
apiVersion: tekton.dev/v1alpha1
kind: PipelineResource
metadata:
  name: rhte-image
spec:
  type: image
  params:
  - name: url
    value: image-registry.openshift-image-registry.svc:5000/rhte-app-${GUID}/rhte-app:latest
EOF
----
. Note that the image is located in the `rhte-app-CLUSTER2_USER` project, while the PipelineResource will be created in the `rhte-pipeline` project.
. Create the Image Pipeline Resource:
+
[source,sh]
----
oc apply -f $HOME/pipeline/rhte-image.yaml -n rhte-pipeline
----
+
.Sample Output
[source,texinfo]
----
pipelineresource.tekton.dev/rhte-image created
----

=== Test Pipeline Tasks

You can test every task by creating *TaskRun* resources. A TaskRun resource references a *Task*, a *Service Account* to run the task, and inputs to the task.

. First, test the Build task.
.. Create the TaskRun definition to test the Build task:
+
[source,sh]
----
cat << EOF >$HOME/pipeline/taskrun-1-s2i-build.yaml
apiVersion: tekton.dev/v1alpha1
kind: TaskRun
metadata:
  name: s2i-nodejs
spec:
  # Use service account with git and image repo credentials
  serviceAccount: pipeline
  taskRef:
    name: s2i-nodejs
  inputs:
    resources:
    - name: source
      resourceRef:
        name: rhte-git
    params:
    - name: TLSVERIFY
      value: "false"
    - name: VERSION
      value: "8"
  outputs:
    resources:
    - name: image
      resourceRef:
        name: rhte-image
EOF
----

.. Note the parameters provided to the Task: The input to the tasks is the `rhte-git` PipelineResource, and the output is the `rhte-image` resource.

.. Create the taskrun. This immediately executes the task:
+
[source,sh]
----
oc apply -f $HOME/pipeline/taskrun-1-s2i-build.yaml -n rhte-pipeline
----
+
.Sample Output
[source,texinfo]
----
taskrun.tekton.dev/s2i-nodejs created
----
.. Taskruns are executed as Pods in OpenShift. Each step in the Task maps into a container in the pod. You can look at the pod itself, but OpenShift Pipelines also provides a CLI tool to directly look at logs and other properties of TaskRuns and PipelineRuns. Using the `tkn` tool, you can see the aggregate logs of all of the containers in the build.
+
Follow along the build (it may take a few minutes for the pod to initialize because it needs to download 7 container images):
+
[source,sh]
----
tkn taskrun logs -f s2i-nodejs
----
+
.Sample Output
[source,texinfo]
----
[git-source-rhte-git-qt5rf] {"level":"warn","ts":1564087797.4948695,"logger":"fallback-logger","caller":"logging/config.go:65","msg":"Fetch GitHub commit ID from kodata failed: \"KO_DATA_PATH\" does not exist or is empty"}
[git-source-rhte-git-qt5rf] {"level":"info","ts":1564087805.1739817,"logger":"fallback-logger","caller":"git/git.go:102","msg":"Successfully cloned https://github.com/wkulhanek/rhte-app.git @ master in path /workspace/source"}

[generate] Application dockerfile generated in /gen-source/Dockerfile.gen

[image-digest-exporter-generate-kdg5k] []

[build] STEP 1: FROM centos/nodejs-10-centos7
[build] Getting image source signatures
[build] Copying blob sha256:497ef6ea0fac8097af3363a9b9032f0948098a9fa2b9002eb51ac65f2ed29cf6

[...]

[push] Copying config sha256:60bb55edc1c4b30419be10f546598cb5febadf74a8a5d5dcdec23bc336ce0da5
[push] Writing manifest to image destination
[push] Storing signatures
[push] Successfully pushed //image-registry.openshift-image-registry.svc:5000/rhte-app-b1fa/rhte-app:latest@sha256:a74498ef67641fb066b7e14f6dbdc2fb5d0938f903fa3eaa66ef50fc4ed510ca

[image-digest-exporter-push-z6pvv] 2019/08/15 00:53:32 ImageResource rhte-image doesn't have an index.json file: stat /builder/home/image-outputs/image/index.json: no such file or directory
[image-digest-exporter-push-z6pvv] 2019/08/15 00:53:32 Image digest exporter output: []
----

.. Validate that the image was built in the `rhte-app` project:
+
[source,sh]
----
oc get is -n rhte-app-${GUID}
----
+
.Sample Output
[source,texinfo,options=nowrap]
----
NAME       IMAGE REPOSITORY                                                          TAGS     UPDATED
rhte-app   image-registry.openshift-image-registry.svc:5000/rhte-app-b1fa/rhte-app   latest   5 minutes ago
----

. Next, test the Image Tagging task.
.. Create the TaskRun definition `TAG=1.0` as the tag of the image:
+
[source,sh]
----
export TAG=1.0
cat << EOF >$HOME/pipeline/taskrun-2-tag-image.yaml
apiVersion: tekton.dev/v1alpha1
kind: TaskRun
metadata:
  name: tag-image
spec:
  serviceAccount: pipeline
  taskRef:
    name: openshift-client
  inputs:
    params:
    - name: ARGS
      value: "tag rhte-app:latest rhte-app:$TAG -n rhte-app-${GUID}"
EOF
----

.. Note the parameters provided to the Task. The input to the tasks is simply the command line arguments to the OpenShift CLI.

.. Create the taskrun:
+
[source,sh]
----
oc apply -f $HOME/pipeline/taskrun-2-tag-image.yaml -n rhte-pipeline
----
+
.Sample Output
[source,texinfo]
----
taskrun.tekton.dev/tag-image created
----
.. Follow along the build:
+
[source,sh]
----
tkn taskrun logs -f tag-image
----
+
.Sample Output
[source,texinfo]
----
[oc] Tag rhte-app:1.0 set to rhte-app@sha256:c6434fa736d2a16a3e439e44c33aef1dce4fd1e824782dfe082463404f231dd2.

[nop] Build successful
----

.. Validate that the image now has tag 1.0:
+
[source,sh]
----
oc get is -n rhte-app-${GUID}
----
+
.Sample Output
[source,texinfo,options=nowrap]
----
NAME       IMAGE REPOSITORY                                                          TAGS         UPDATED
rhte-app   image-registry.openshift-image-registry.svc:5000/rhte-app-b1fa/rhte-app   1.0,latest   About a minute ago
----

. Next, test the Image Copying task.
.. Create the TaskRun definition `TAG=1.0` as the tag of the image. Also set `QUAY_USER` to _your_ Quay user ID. This is _not_ the robot account, but your user ID. You need that because your repository in Quay is in your personal account:
+
[source,sh]
----
export TAG=1.0
export QUAY_USER=wkulhanek

cat << EOF >$HOME/pipeline/taskrun-3-skopeo.yaml
apiVersion: tekton.dev/v1alpha1
kind: TaskRun
metadata:
  name: copy-to-quay
spec:
  serviceAccount: pipeline
  taskRef:
    name: skopeo
  inputs:
    params:
    - name: ARGS
      value: "copy --src-tls-verify=false docker://image-registry.openshift-image-registry.svc:5000/rhte-app-$GUID/rhte-app:$TAG docker://quay-common.apps.cluster-common.common.events.opentlc.com/$QUAY_USER/rhte-app:$TAG"
EOF
----

.. Note the parameters provided to the Task. The input to the tasks is simply the command line arguments to `skopeo`.

.. Create the taskrun:
+
[source,sh]
----
oc apply -f $HOME/pipeline/taskrun-3-skopeo.yaml -n rhte-pipeline
----
+
.Sample Output
[source,texinfo]
----
taskrun.tekton.dev/copy-to-quay created
----
.. Follow along the build:
+
[source,sh]
----
tkn taskrun logs -f copy-to-quay
----
+
.Sample Output
[source,texinfo,options=nowrap]
----
[skopeo] Getting image source signatures
[skopeo] Copying blob sha256:9097e153bf940b0ed05910d703ca5ef049ba48241a397649e24581f8f1eb5bfe
[skopeo] Copying blob sha256:ee6b95d93e4ec1b3cfe870fbf976bb2f474dcc7a62c21c1c934e3018dc50edb8
[skopeo] Copying blob sha256:7d6351d37f23b8de7abc715ca89ead935630634f5d418a67e0fb6e81adb13a2c
[skopeo] Copying blob sha256:92cc4d8eb1ee6ea21a5d28014d853cc2bac191bc3fd4fb9737fa90439eed1c31
[skopeo] Copying blob sha256:7d0b324847a822ccbbb1fc49a1b0c369f99f934f52bd1b947c4c54dbb6bf38f3
[skopeo] Copying blob sha256:ba43a96d4c09d7111bae423c69de41a76212f911b647502e1748a8b28b0dc446
[skopeo] Copying blob sha256:f41df985143af3f5b5728663bb40668f22b9a42b07d7ad568a775e15caeb6f1c
[skopeo] Copying blob sha256:8edbe0b7b44b861eeee18bfdefbd0a3781fca9b26d8d07bbf5c8767c9b44b49c
[skopeo] Copying config sha256:60bb55edc1c4b30419be10f546598cb5febadf74a8a5d5dcdec23bc336ce0da5
[skopeo] Writing manifest to image destination
[skopeo] Storing signatures
----

.. In your web browser, navigate to link:https://quay-common.apps.cluster-common.common.events.opentlc.com[https://quay-common.apps.cluster-common.common.events.opentlc.com^] and check that your repository now has an image in it with tag 1.0.

. Finally, test setting the image in the Federated Deployment.
.. Create the TaskRun definition:
+
[source,sh]
----
export TAG=1.0
export QUAY_USER=wkulhanek

cat << EOF >$HOME/pipeline/taskrun-4-set-image.yaml
apiVersion: tekton.dev/v1alpha1
kind: TaskRun
metadata:
  name: set-image
spec:
  # Use service account with git and image repo credentials
  serviceAccount: pipeline
  taskRef:
    name: patch
  inputs:
    params:
    - name: RESOURCE
      value: FederatedDeployment
    - name: RESOURCE_NAME
      value: rhte-app
    - name: NAMESPACE
      value: rhte-app-${GUID}
    - name: TYPE
      value: merge
    - name: PATCH
      value: '{"spec":{"template":{"spec":{"template":{"spec":{"containers":[{"env":[{"name":"CLUSTER_NAME","value":"TBD"},{"name":"IMAGE_TAG","value":"$TAG"},{"name":"PREFIX","value":"TBD"}],"image":"quay-common.apps.cluster-common.common.events.opentlc.com/$QUAY_USER/rhte-app:$TAG","name":"rhte-app", "ports":[{"containerPort":3000}]}]}}}}}}'
EOF
----

.. Note the parameters provided to the Task. The input to the tasks contains the type of resource, resource name, namespace, merge type, and patch string.

.. Create the taskrun:
+
[source,sh]
----
oc apply -f $HOME/pipeline/taskrun-4-set-image.yaml -n rhte-pipeline
----
+
.Sample Output
[source,texinfo]
----
taskrun.tekton.dev/set-image created
----
.. Follow along the build:
+
[source,sh]
----
tkn taskrun logs -f set-image
----
+
.Sample Output
[source,texinfo]
----
[patch] federateddeployment.types.kubefed.io/rhte-app patched
----

.. Verify that the Federated Deployment has updated the deployment with the new image:
+
[source,sh]
----
oc describe deployment rhte-app -n rhte-app-${GUID}|grep -i image
----
+
.Sample Output
[source,texinfo,options=nowrap]
----
    Image:      quay-common.apps.cluster-common.common.events.opentlc.com/wkulhanek/rhte-app:1.0
      IMAGE_TAG:     1.0
----

.. Remind yourself of the URL of your application:
+
[source,sh]
----
oc get route -n rhte-app-${GUID}
----
+
.. Using the route to your application, validate in a web browser that the placeholder application has been replaced with the real application. This application now reads the Environment Variables from the Pod and displays them. You should see the following:

* You are on Cluster: *Cluster 1*
* Image Tag for this application: *1.0*
* Your project prefix: *_<Your GUID>_*

. This concludes the tests.

=== Create and Run Pipeline

Now that all of the tests have succeeded, you are ready to create and run the pipeline.

. Create the pipeline YAML definition. This time you are using `TAG=2.0`, because you want to see the new tag being applied:
+
[NOTE]
====
In the future, there may be a way to set this via a PipelineResource--but currently this does not seem possible. Also note that you are hard-coding the namespace for the same reason.
====
+
[source,sh]
----
export TAG=2.0
export QUAY_USER=wkulhanek

cat << EOF >$HOME/pipeline/rhte-pipeline.yaml
apiVersion: tekton.dev/v1alpha1
kind: Pipeline
metadata:
  name: rhte-pipeline
spec:
  resources:
  - name: app-repository
    type: git
  - name: app-image
    type: image
  tasks:
  - name: build
    taskRef:
      name: s2i-nodejs
    params:
      - name: TLSVERIFY
        value: "false"
      - name: VERSION
        value: "8"
    resources:
      inputs:
      - name: source
        resource: app-repository
      outputs:
      - name: image
        resource: app-image
  - name: tag-image
    taskRef:
      name: openshift-client
    runAfter:
      - build
    params:
    - name: ARGS
      value: "tag rhte-app:latest rhte-app:$TAG -n rhte-app-${GUID}"
  - name: copy-image
    taskRef:
      name: skopeo
    runAfter:
      - tag-image
    params:
    - name: ARGS
      value: "copy --src-tls-verify=false docker://image-registry.openshift-image-registry.svc:5000/rhte-app-${GUID}/rhte-app:$TAG docker://quay-common.apps.cluster-common.common.events.opentlc.com/$QUAY_USER/rhte-app:$TAG"
  - name: deploy-image
    taskRef:
      name: patch
    runAfter:
      - copy-image
    params:
    - name: RESOURCE
      value: FederatedDeployment
    - name: RESOURCE_NAME
      value: rhte-app
    - name: NAMESPACE
      value: rhte-app-${GUID}
    - name: TYPE
      value: merge
    - name: PATCH
      value: '{"spec":{"template":{"spec":{"template":{"spec":{"containers":[{"env":[{"name":"CLUSTER_NAME","value":"TBD"},{"name":"IMAGE_TAG","value":"$TAG"},{"name":"PREFIX","value":"TBD"}],"image":"quay-common.apps.cluster-common.common.events.opentlc.com/$QUAY_USER/rhte-app:$TAG","name":"rhte-app", "ports":[{"containerPort":3000}]}]}}}}}}'
EOF
----

. Create the pipeline:
+
[source,sh]
----
oc apply -f $HOME/pipeline/rhte-pipeline.yaml -n rhte-pipeline
----
+
.Sample Output
[source,texinfo]
----
pipeline.tekton.dev/rhte-pipeline created
----

. Now that you have the pipeline in OpenShift, you can create a PipelineRun to execute the pipeline. This PipelineRun resource provides the inputs for the pipeline. As noted above, ideally, the tag and namespace would also come from PipelineResources--but as of the writing of this lab, that does not seem possible. Therefore, those settings are specified in the Pipeline resource itself.
+
Create the PipelineRun definition:
+
[source,sh]
----
cat << EOF >$HOME/pipeline/rhte-pipelinerun.yaml
apiVersion: tekton.dev/v1alpha1
kind: PipelineRun
metadata:
  # Usually this would be generateName to generate
  # a unique name
  name: rhte-pipelinerun
spec:
  pipelineRef:
    name: rhte-pipeline
  trigger:
    type: manual
  serviceAccount: 'pipeline'
  resources:
  - name: app-repository
    resourceRef:
      name: rhte-git
  - name: app-image
    resourceRef:
      name: rhte-image
EOF
----

. Note that normally you would use `generateName` instead of `name` in the `metadata` section to generate a new `pipelinerun` name every time you created this object. But for the purposes of this lab, executing one pipeline run will be enough.

. Create the `pipelinerun`:
+
[source,sh]
----
oc apply -f $HOME/pipeline/rhte-pipelinerun.yaml
----
+
.Sample Output
[source,texinfo]
----
pipelinerun.tekton.dev/rhte-pipelinerun created
----
+
As before with TaskRuns, creating the PipelineRun immediately starts the pipeline.
. List the current pipeline runs:
+
[source,sh]
----
tkn pr list
----
+
.Sample Output
[source,texinfo]
----
NAME               STARTED          DURATION   STATUS
rhte-pipelinerun   37 seconds ago   ---        Running
----

. Tail the logs for the pipeline run. These logs should look familiar--they are the combination of all of the individual task runs that you executed earlier.
+
[source,sh]
----
tkn pr logs -f rhte-pipelinerun
----
+
.Sample Output
[source,texinfo,options=nowrap]
----
[build : git-source-rhte-git-6l8xt] {"level":"warn","ts":1565835054.7675672,"logger":"fallback-logger","caller":"logging/config.go:69","msg":"Fetch GitHub commit ID from kodata failed: \"KO_DATA_PATH\" does not exist or is empty"}
[build : git-source-rhte-git-6l8xt] {"level":"info","ts":1565835057.2689905,"logger":"fallback-logger","caller":"git/git.go:102","msg":"Successfully cloned https://github.com/wkulhanek/rhte-app.git @ master in path /workspace/source"}

[build : generate] Application dockerfile generated in /gen-source/Dockerfile.gen

[build : image-digest-exporter-generate-49l8d] 2019/08/15 02:10:58 ImageResource rhte-image doesn't have an index.json file: stat /builder/home/image-outputs/image/index.json: no such file or directory
[build : image-digest-exporter-generate-49l8d] 2019/08/15 02:10:58 Image digest exporter output: []

[build : build] STEP 1: FROM registry.access.redhat.com/rhscl/nodejs-8-rhel7

[....]

[tag-image : oc] Tag rhte-app:2.0 set to rhte-app@sha256:0a293b5abf1958b2f0af154b2c6ff510f7e0b04e279e483eba10360a279191b4.

[copy-image : skopeo] Getting image source signatures
[copy-image : skopeo] Copying blob sha256:92cc4d8eb1ee6ea21a5d28014d853cc2bac191bc3fd4fb9737fa90439eed1c31
[copy-image : skopeo] Copying blob sha256:7d6351d37f23b8de7abc715ca89ead935630634f5d418a67e0fb6e81adb13a2c
[copy-image : skopeo] Copying blob sha256:ba43a96d4c09d7111bae423c69de41a76212f911b647502e1748a8b28b0dc446
[copy-image : skopeo] Copying blob sha256:7d0b324847a822ccbbb1fc49a1b0c369f99f934f52bd1b947c4c54dbb6bf38f3
[copy-image : skopeo] Copying blob sha256:ee6b95d93e4ec1b3cfe870fbf976bb2f474dcc7a62c21c1c934e3018dc50edb8
[copy-image : skopeo] Copying blob sha256:edead4452eb71ce818204403f8564ed24f2ada84d1899a740056267f10d4692f
[copy-image : skopeo] Copying blob sha256:3ebf580ffbdc58c3bcf1d04a08ce00022a97fba999cff95d928774299b0e77e9
[copy-image : skopeo] Copying blob sha256:899ca3ad627f9e31e207f9ab1632486b1a25e8d0eda961c0d5f64109e04c8b6f
[copy-image : skopeo] Copying config sha256:925caa0add002f08aa734047291ae7993393395246e39cd4aaa5cd4e75cd1459
[copy-image : skopeo] Writing manifest to image destination
[copy-image : skopeo] Storing signatures

[deploy-image : patch] federateddeployment.types.kubefed.k8s.io/rhte-app patched
----

. Your pipeline has executed.
. Check the status of your pipeline run:
+
[source,sh]
----
tkn pr list
----
+
.Sample Output
[source,texinfo]
----
NAME               STARTED         DURATION    STATUS
rhte-pipelinerun   2 minutes ago   2 minutes   Succeeded
----

. The PipelineRun created a TaskRun object for every task in the pipeline. Check the task runs:
+
[source,sh]
----
tkn tr list
----
+
.Sample Output
[source,texinfo,options=nowrap]
----
NAME                                  STARTED          DURATION     STATUS
rhte-pipelinerun-deploy-image-42wql   55 seconds ago   17 seconds   Succeeded
rhte-pipelinerun-copy-image-c72fj     1 minute ago     28 seconds   Succeeded
rhte-pipelinerun-tag-image-hx6ml      1 minute ago     17 seconds   Succeeded
rhte-pipelinerun-build-rwbp6          5 minutes ago    3 minutes    Succeeded
set-image                             9 minutes ago    16 seconds   Succeeded
copy-to-quay                          12 minutes ago   52 seconds   Succeeded
tag-image                             19 minutes ago   38 seconds   Succeeded
s2i-nodejs                            1 hour ago       5 minutes    Succeeded
----
+
Note the individual test task runs that you created earlier as well as the four task runs starting with `rhte-pipelinerun` that the pipeline created.

. Double-check that the application is now using version 2.0 of the image:
+
[source,sh]
----
oc describe deployment rhte-app -n rhte-app-${GUID}|grep -i image
----
+
.Sample Output
[source,texinfo]
----
    Image:      quay-common.apps.cluster-common.common.events.opentlc.com/wkulhanek/rhte-app:2.0
      IMAGE_TAG:     2.0
----

. Finally, in a web browser, navigate to the route for the application both on cluster 1 and on cluster 2. The web application should show the following settings now:

* You are on Cluster: *Cluster 1*
* Image Tag for this application: *2.0*
* Your project prefix: *_<Your GUID>_*

. And on cluster 2 (remember that the route on cluster 2 is `http://rhte-app-<YOUR GUID>.apps.cluster-common.common.events.opentlc.com`):

* You are on Cluster: *Cluster 2*
* Image Tag for this application: *2.0*
* Your project prefix: *common*

== Cleanup

To conserve resources on the shared cluster for the next time we run this lab please delete your federated project:

[source,sh]
----
oc delete project rhte-app-${GUID}
----

== Wrap-up

Congratulations! You have completed this lab.

In this lab, you:

* Set up a Federated Cluster consisting of two OpenShift clusters
* Deployed an application to two clusters using *KubeFed*
* Registered for an account on a private Quay container image registry
* Used *OpenShift Pipelines* to build an updated container image for the application and redeploy it to both clusters using KubeFed and the Quay image registry
